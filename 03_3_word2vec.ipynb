{"cells":[{"cell_type":"markdown","id":"3a564d5f","metadata":{"id":"3a564d5f"},"source":["#  Word2vec\n","\n","__Автор задач: Блохин Н.В. (NVBlokhin@fa.ru)__\n","\n","Материалы:\n","* Deep Learning with PyTorch (2020) Авторы: Eli Stevens, Luca Antiga, Thomas Viehmann\n","* https://radimrehurek.com/gensim/models/word2vec.html\n","* https://radimrehurek.com/gensim/auto_examples/tutorials/run_word2vec.html\n","* https://pytorch.org/text/stable/vocab.html\n","* https://github.com/OlgaChernytska/word2vec-pytorch\n","* https://www.baeldung.com/cs/nlps-word2vec-negative-sampling\n","* https://towardsdatascience.com/implementing-word2vec-in-pytorch-from-the-ground-up-c7fe5bf99889"]},{"cell_type":"markdown","id":"c9ecd663","metadata":{"id":"c9ecd663"},"source":["## Задачи для совместного разбора"]},{"cell_type":"markdown","id":"1843b512","metadata":{"id":"1843b512"},"source":["1\\. Рассмотрите основные шаги подготовки данных для обучения skip-gram модели"]},{"cell_type":"code","execution_count":null,"id":"31545f32","metadata":{"id":"31545f32"},"outputs":[],"source":["text = \"Спящий котик мило моргает своими яркими глазками\""]},{"cell_type":"code","source":["from torchtext.vocab import build_vocab_from_iterator"],"metadata":{"id":"DIvQNgftd1gs"},"id":"DIvQNgftd1gs","execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"9ZyxzckOeWdQ"},"id":"9ZyxzckOeWdQ","execution_count":null,"outputs":[]},{"cell_type":"code","source":["corpus = [\n","    text.lower().split()\n","]\n","vocab = build_vocab_from_iterator(corpus)"],"metadata":{"id":"6hsqkbUeeEY4"},"id":"6hsqkbUeeEY4","execution_count":null,"outputs":[]},{"cell_type":"code","source":["corpus_i = [\n","    vocab.lookup_indices(t)\n","    for t in corpus\n","]\n","corpus_i"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"aBwIDkUJeXd6","executionInfo":{"status":"ok","timestamp":1697627367492,"user_tz":-180,"elapsed":271,"user":{"displayName":"Delovaya Kolbasa","userId":"08122373828115757783"}},"outputId":"39d65248-25a4-4553-ddfb-56c34b35b98a"},"id":"aBwIDkUJeXd6","execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[[5, 1, 2, 3, 4, 6, 0]]"]},"metadata":{},"execution_count":5}]},{"cell_type":"code","source":["ex = corpus_i[0]\n","input, output = [], []\n","for idx, word in enumerate(ex):\n","  if idx == 0 or idx == len(ex) - 1:\n","    continue\n","  input.append(word)\n","  output.append(ex[idx - 1])\n","  input.append(word)\n","  output.append(ex[idx + 1])"],"metadata":{"id":"Mbxneptueed8"},"id":"Mbxneptueed8","execution_count":null,"outputs":[]},{"cell_type":"code","source":["import torch as th\n","\n","input = th.tensor(input, dtype=th.long)\n","output = th.tensor(output, dtype=th.long)"],"metadata":{"id":"nA-upj86fKDS"},"id":"nA-upj86fKDS","execution_count":null,"outputs":[]},{"cell_type":"markdown","id":"98ad623c","metadata":{"id":"98ad623c"},"source":["2\\. Рассмотрите основные шаги по настройке skip-gram модели"]},{"cell_type":"code","source":["import torch.nn as nn\n","\n","emb = nn.Embedding(len(vocab), 300)"],"metadata":{"id":"bN-KevdYd_WJ"},"id":"bN-KevdYd_WJ","execution_count":null,"outputs":[]},{"cell_type":"code","source":["len(input), emb(input).shape"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"zTDeU2hWgAt1","executionInfo":{"status":"ok","timestamp":1697627790964,"user_tz":-180,"elapsed":5,"user":{"displayName":"Delovaya Kolbasa","userId":"08122373828115757783"}},"outputId":"1948d54f-11c8-42d6-96a7-ba0849400e1c"},"id":"zTDeU2hWgAt1","execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(10, torch.Size([10, 300]))"]},"metadata":{},"execution_count":15}]},{"cell_type":"code","source":["fc = nn.Linear(in_features=300, out_features=len(vocab))"],"metadata":{"id":"N7wJ-4M7gY-i"},"id":"N7wJ-4M7gY-i","execution_count":null,"outputs":[]},{"cell_type":"code","source":["x_e = emb(input)\n","out = fc(x_e)\n","out.shape"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"zlQDj2lngiyR","executionInfo":{"status":"ok","timestamp":1697627936045,"user_tz":-180,"elapsed":3,"user":{"displayName":"Delovaya Kolbasa","userId":"08122373828115757783"}},"outputId":"4825f1b6-d4b7-4716-bf31-3682a19ae5fe"},"id":"zlQDj2lngiyR","execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["torch.Size([10, 7])"]},"metadata":{},"execution_count":17}]},{"cell_type":"code","source":["model = nn.Sequential(\n","    nn.Embedding(len(vocab), 300),\n","    nn.Linear(in_features=300, out_features=len(vocab))\n",")\n","crit = nn.CrossEntropyLoss()\n","out = model(input)\n","loss = crit(out, output)\n","loss"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"hRM6dEMDgt0P","executionInfo":{"status":"ok","timestamp":1697628067637,"user_tz":-180,"elapsed":259,"user":{"displayName":"Delovaya Kolbasa","userId":"08122373828115757783"}},"outputId":"e1f7c5a9-df2e-49bc-c458-55e2b5cc781e"},"id":"hRM6dEMDgt0P","execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor(2.0615, grad_fn=<NllLossBackward0>)"]},"metadata":{},"execution_count":18}]},{"cell_type":"code","source":["emb = nn.Embedding(len(vocab), 16, max_norm=1)\n","inputs_e = emb(input)\n","outputs_e = emb(output)"],"metadata":{"id":"CW89ni-bhul1"},"id":"CW89ni-bhul1","execution_count":null,"outputs":[]},{"cell_type":"code","source":["inputs_e[0] @ outputs_e[0]"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"-EWne8jIiWt2","executionInfo":{"status":"ok","timestamp":1697628481030,"user_tz":-180,"elapsed":4,"user":{"displayName":"Delovaya Kolbasa","userId":"08122373828115757783"}},"outputId":"31c00265-4ce0-4f72-a56f-24e9c1b3475f"},"id":"-EWne8jIiWt2","execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor(0.0706, grad_fn=<DotBackward0>)"]},"metadata":{},"execution_count":23}]},{"cell_type":"code","source":["(inputs_e @ outputs_e.T).shape"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"RRJUWYGxjGXB","executionInfo":{"status":"ok","timestamp":1697628617720,"user_tz":-180,"elapsed":4,"user":{"displayName":"Delovaya Kolbasa","userId":"08122373828115757783"}},"outputId":"5d354356-7596-4737-fe30-6b8a0ed8dd98"},"id":"RRJUWYGxjGXB","execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["torch.Size([10, 10])"]},"metadata":{},"execution_count":25}]},{"cell_type":"code","source":["o = inputs_e.view(-1, 1, 16).bmm(outputs_e.view(-1, 16, 1))\n","o"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"FQTq-bcSjQLl","executionInfo":{"status":"ok","timestamp":1697628663816,"user_tz":-180,"elapsed":349,"user":{"displayName":"Delovaya Kolbasa","userId":"08122373828115757783"}},"outputId":"b83f8cd4-949a-47b2-f9a7-12668f69f5ed"},"id":"FQTq-bcSjQLl","execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([[[ 0.0706]],\n","\n","        [[ 0.0801]],\n","\n","        [[ 0.0801]],\n","\n","        [[-0.1142]],\n","\n","        [[-0.1142]],\n","\n","        [[ 0.0977]],\n","\n","        [[ 0.0977]],\n","\n","        [[-0.4377]],\n","\n","        [[-0.4377]],\n","\n","        [[-0.1486]]], grad_fn=<BmmBackward0>)"]},"metadata":{},"execution_count":26}]},{"cell_type":"markdown","id":"4d7b6d63","metadata":{"id":"4d7b6d63"},"source":["## Задачи для самостоятельного решения"]},{"cell_type":"code","source":["import pandas as pd\n","import nltk\n","import pymorphy2\n","from nltk import sent_tokenize, RegexpTokenizer, word_tokenize\n","import re\n","from nltk.corpus import stopwords\n","from torchtext.vocab import build_vocab_from_iterator\n","\n","\n","nltk.download('punkt')\n","nltk.download('stopwords')\n","sw = stopwords.words('russian')"],"metadata":{"id":"mUB0VZrqqkaG","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1698577328736,"user_tz":-180,"elapsed":8144,"user":{"displayName":"Delovaya Kolbasa","userId":"08122373828115757783"}},"outputId":"b9a64c9f-ea4a-4c07-ecb4-3cfc97d3b089"},"id":"mUB0VZrqqkaG","execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Package punkt is already up-to-date!\n","[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Package stopwords is already up-to-date!\n"]}]},{"cell_type":"markdown","id":"604e329f","metadata":{"id":"604e329f"},"source":["<p class=\"task\" id=\"1\"></p>\n","\n","1\\. Загрузите тексты новостей из файла `news_500.csv`. Удалите из текстов все знаки препинания и символы не из русского алфавита, приведите все слова к нижнему регистру и удалите стоп-слова. Разбейте текст каждой новости, удалив из них стоп-слова. Разбейте текст каждой новости на фрагменты по 3 предложения и сохраните в виде списка строк. Выведите на экран длину полученного списка.\n","\n","- [x] Проверено на семинаре"]},{"cell_type":"code","source":["def processing(text):\n","  tokenizer = RegexpTokenizer(r'\\w+')\n","  sents = sent_tokenize(text)\n","  sents_split = [sents[i:i+3] for i in range(0, len(sents), 3)]\n","  res = []\n","  for s in sents_split:\n","    words = []\n","    s_buf = ' '.join(s).lower()\n","    s_pure = tokenizer.tokenize(re.sub(r'[^А-Яа-я ]', ' ', s_buf))\n","    for word in s_pure:\n","      if word not in sw:\n","        words.append(word)\n","    res.append(' '.join(words))\n","  return res"],"metadata":{"id":"LvrCbyUnrU9_"},"id":"LvrCbyUnrU9_","execution_count":null,"outputs":[]},{"cell_type":"code","source":["df = pd.read_csv('news_500.csv', usecols=['text'])\n","proc = df['text'].apply(processing)\n","corpus = proc.sum()"],"metadata":{"id":"ohMy7q2Gqw8W"},"id":"ohMy7q2Gqw8W","execution_count":null,"outputs":[]},{"cell_type":"code","source":["len(corpus)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"E6scvxx8q58j","executionInfo":{"status":"ok","timestamp":1697631903324,"user_tz":-180,"elapsed":4,"user":{"displayName":"Delovaya Kolbasa","userId":"08122373828115757783"}},"outputId":"556e8b7f-c4e0-4edc-d4cc-e418e45ade66"},"id":"E6scvxx8q58j","execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["1965"]},"metadata":{},"execution_count":59}]},{"cell_type":"markdown","id":"49cb99c9","metadata":{"id":"49cb99c9"},"source":["<p class=\"task\" id=\"2\"></p>\n","\n","2\\. Настройте модель Word2Vec из пакета `gensim`. Для валидации выведите на экран информацию о ближайших словах для нескольких случайно выбранных токенов из обучающей выборки.\n","\n","- [ ] Проверено на семинаре"]},{"cell_type":"code","source":["corpus_t = [word_tokenize(s) for s in corpus]"],"metadata":{"id":"q8VpU0haimpi"},"id":"q8VpU0haimpi","execution_count":null,"outputs":[]},{"cell_type":"code","source":["from gensim.test.utils import common_texts\n","from gensim.models import Word2Vec\n","\n","model = Word2Vec(sentences=corpus_t, vector_size=100, window=5, min_count=1, workers=4)"],"metadata":{"id":"fHqsgcksgUmw"},"id":"fHqsgcksgUmw","execution_count":null,"outputs":[]},{"cell_type":"code","source":["from random import randint\n","rand_words = []\n","for _ in range(2):\n","  ind1 = randint(0, len(corpus_t)-1)\n","  ind2 = randint(0, len(corpus_t[ind1])-1)\n","  rand_words.append(corpus_t[ind1][ind2])\n","rand_words"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ExxSfEeDj0b8","executionInfo":{"status":"ok","timestamp":1698049678526,"user_tz":-180,"elapsed":856,"user":{"displayName":"Delovaya Kolbasa","userId":"08122373828115757783"}},"outputId":"1ee1e485-0332-4ba3-abfb-626262781fc4"},"id":"ExxSfEeDj0b8","execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["['могло', 'дефицита']"]},"metadata":{},"execution_count":17}]},{"cell_type":"code","source":["for word in rand_words:\n","  print(f'{word}\\n{model.wv.most_similar(word, topn=5)}')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ulW-ruoMjKPV","executionInfo":{"status":"ok","timestamp":1698049682645,"user_tz":-180,"elapsed":209,"user":{"displayName":"Delovaya Kolbasa","userId":"08122373828115757783"}},"outputId":"31e80934-9c41-4fac-b774-351f08e8205e"},"id":"ulW-ruoMjKPV","execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["могло\n","[('информации', 0.6751124262809753), ('г', 0.6723748445510864), ('составляет', 0.6688089370727539), ('около', 0.6651775240898132), ('недели', 0.6649465560913086)]\n","дефицита\n","[('валютным', 0.4208981394767761), ('суриков', 0.42014119029045105), ('основной', 0.39718472957611084), ('канадских', 0.38568130135536194), ('афганистан', 0.38430055975914)]\n"]}]},{"cell_type":"markdown","id":"d22ce44c","metadata":{"id":"d22ce44c"},"source":["<p class=\"task\" id=\"3\"></p>\n","\n","3\\. Опишите класс `W2VDataset`, который реализует в себе логику получения контекстного окна для обучения skip-gram модели. При создании словаря игнорируйте токены, которые встретились меньше 20 раз. Продемонстрируйте пример работы.\n","\n","![image.png](attachment:image.png)\n","\n","- [ ] Проверено на семинаре"]},{"cell_type":"code","source":["class W2VDataset():\n","  def __init__(self, corpus):\n","    corpus_t = [word_tokenize(s) for s in corpus]\n","    self.vocab = build_vocab_from_iterator(\n","        corpus_t, min_freq=20, specials=['<PAD>', '<UNK>']\n","    )\n","    self.vocab.set_default_index(self.vocab['<UNK>'])\n","\n","  def context(self, batch, window=3):\n","    input, output = [], []\n","    gap = window // 2\n","    batch_i = [\n","        self.vocab.lookup_indices(tokens)\n","        for tokens in batch\n","    ]\n","    for corpus_i in batch_i:\n","      for idx, word in enumerate(corpus_i):\n","        if idx < gap or idx > len(corpus_i) - gap - 1:\n","          continue\n","        input.extend((window - 1)*[word])\n","        output.extend(corpus_i[idx - gap: idx])\n","        output.extend(corpus_i[idx + 1: idx + gap + 1])\n","    return input, output"],"metadata":{"id":"6YInC3-9nkYk"},"id":"6YInC3-9nkYk","execution_count":null,"outputs":[]},{"cell_type":"code","source":["dataset = W2VDataset(corpus)\n","corpus_t = [word_tokenize(s) for s in corpus]\n","dataset.context([corpus_t[0]], window=3)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"sfInmTQ9e-8g","executionInfo":{"status":"ok","timestamp":1698577377628,"user_tz":-180,"elapsed":3696,"user":{"displayName":"Delovaya Kolbasa","userId":"08122373828115757783"}},"outputId":"3b7609f2-26ca-44ee-898f-7186e8ea9bbd"},"id":"sfInmTQ9e-8g","execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["([3,\n","  3,\n","  54,\n","  54,\n","  125,\n","  125,\n","  36,\n","  36,\n","  1,\n","  1,\n","  1,\n","  1,\n","  1,\n","  1,\n","  154,\n","  154,\n","  1,\n","  1,\n","  1,\n","  1,\n","  173,\n","  173,\n","  320,\n","  320,\n","  1,\n","  1,\n","  1,\n","  1,\n","  348,\n","  348,\n","  49,\n","  49,\n","  73,\n","  73,\n","  4,\n","  4,\n","  20,\n","  20,\n","  2,\n","  2,\n","  104,\n","  104,\n","  33,\n","  33,\n","  125,\n","  125,\n","  5,\n","  5,\n","  12,\n","  12,\n","  424,\n","  424,\n","  1,\n","  1,\n","  154,\n","  154,\n","  1,\n","  1,\n","  1,\n","  1,\n","  1,\n","  1,\n","  1,\n","  1,\n","  118,\n","  118,\n","  11,\n","  11,\n","  1,\n","  1,\n","  1,\n","  1],\n"," [50,\n","  54,\n","  3,\n","  125,\n","  54,\n","  36,\n","  125,\n","  1,\n","  36,\n","  1,\n","  1,\n","  1,\n","  1,\n","  154,\n","  1,\n","  1,\n","  154,\n","  1,\n","  1,\n","  173,\n","  1,\n","  320,\n","  173,\n","  1,\n","  320,\n","  1,\n","  1,\n","  348,\n","  1,\n","  49,\n","  348,\n","  73,\n","  49,\n","  4,\n","  73,\n","  20,\n","  4,\n","  2,\n","  20,\n","  104,\n","  2,\n","  33,\n","  104,\n","  125,\n","  33,\n","  5,\n","  125,\n","  12,\n","  5,\n","  424,\n","  12,\n","  1,\n","  424,\n","  154,\n","  1,\n","  1,\n","  154,\n","  1,\n","  1,\n","  1,\n","  1,\n","  1,\n","  1,\n","  118,\n","  1,\n","  11,\n","  118,\n","  1,\n","  11,\n","  1,\n","  1,\n","  4])"]},"metadata":{},"execution_count":5}]},{"cell_type":"markdown","id":"2baa4164","metadata":{"id":"2baa4164"},"source":["<p class=\"task\" id=\"4\"></p>\n","\n","4\\. Реализуйте и настройте skip-gram модель. Перед началом обучения выберите случайным образом несколько слов из датасета и для каждого из них выведите на экран 3 ближайших слова в смысле косинусной близости между эмбеддингами. В процессе настройки для валидации периодически выводите на экран информацию о ближайших словах для этих слов. Выведите на экран график значения функции потерь в зависимости от номера эпохи.  \n","\n","![image.png](attachment:image.png)\n","\n","- [ ] Проверено на семинаре"]},{"cell_type":"code","source":["import torch as th\n","import torch.nn as nn\n","import torch.optim as optim\n","from torch.utils.data.dataloader import DataLoader"],"metadata":{"id":"ZT--IhWRtefW"},"id":"ZT--IhWRtefW","execution_count":null,"outputs":[]},{"cell_type":"code","source":["class SkipGramModel(nn.Module):\n","  def __init__(self, vocab_len):\n","    super().__init__()\n","    self.emb = nn.Embedding(\n","        num_embeddings=vocab_len,\n","        embedding_dim=300,\n","        padding_idx=0\n","    )\n","    self.fc = nn.Linear(in_features=300, out_features=vocab_len)\n","\n","  def forward(self, X):\n","    e = self.emb(X)\n","    out = self.fc(e)\n","    return out"],"metadata":{"id":"G4alA7DFrIln"},"id":"G4alA7DFrIln","execution_count":null,"outputs":[]},{"cell_type":"code","source":["dataset = W2VDataset(corpus)\n","X, y = dataset.context(corpus_t)\n","X = th.tensor(X).long()\n","y = th.tensor(y).long()\n","X.shape, y.shape"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"qpLp97L6xL8o","executionInfo":{"status":"ok","timestamp":1698577394596,"user_tz":-180,"elapsed":1250,"user":{"displayName":"Delovaya Kolbasa","userId":"08122373828115757783"}},"outputId":"0da312bd-8413-48ba-fb4a-3ecbaba48516"},"id":"qpLp97L6xL8o","execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(torch.Size([137358]), torch.Size([137358]))"]},"metadata":{},"execution_count":8}]},{"cell_type":"code","source":["loader = DataLoader(list(zip(X, y)), batch_size=64, num_workers=4)"],"metadata":{"id":"37dyCbXTfkl6"},"id":"37dyCbXTfkl6","execution_count":null,"outputs":[]},{"cell_type":"code","source":["def words_sim(indices):\n","  for ind in indices:\n","    word_emb = skipgram.emb(ind).reshape(1, -1)\n","    cos_sim = word_emb @ skipgram.emb.weight.T / \\\n","              ((word_emb**2).sum(axis=1)**0.5 * (skipgram.emb.weight**2).sum(axis=1)**0.5)\n","    top = th.argsort(cos_sim[0][1:], descending=True)[:5] + 1\n","    top_words = dataset.vocab.lookup_tokens(list(top))\n","    print(f'{top_words[0]}\\t-> {top_words[1:]}')"],"metadata":{"id":"V51D4ObjdtLn"},"id":"V51D4ObjdtLn","execution_count":null,"outputs":[]},{"cell_type":"code","source":["n_epoch = 10\n","lr = 0.1\n","skipgram = SkipGramModel(len(dataset.vocab))\n","crit = nn.CrossEntropyLoss()\n","optimizer = optim.Adam(skipgram.parameters(), lr=lr)\n","rand_ind = th.randint(len(dataset.vocab), size=(3,))\n","\n","for epoch in range(n_epoch):\n","  # for X_b, y_b in loader:\n","  out = skipgram(X)\n","  loss = crit(out, y)\n","  loss.backward()\n","  optimizer.step()\n","  optimizer.zero_grad()\n","  print(f'{epoch=} {loss.item()=}')\n","  words_sim(rand_ind)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"FaMnlxhLtdky","executionInfo":{"status":"ok","timestamp":1698577824872,"user_tz":-180,"elapsed":238696,"user":{"displayName":"Delovaya Kolbasa","userId":"08122373828115757783"}},"outputId":"01eead91-8650-4bef-db5e-8a20055e94f8"},"id":"FaMnlxhLtdky","execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["epoch=0 loss.item()=6.19916296005249\n","очередь\t-> ['переговоров', 'возможно', 'отказался', 'стало']\n","сегодня\t-> ['стран', 'решения', 'ходе', 'зрения']\n","день\t-> ['условиях', 'накануне', 'идет', 'долларов']\n","epoch=1 loss.item()=10.549703598022461\n","очередь\t-> ['переговоров', 'отказался', 'возможно', 'края']\n","сегодня\t-> ['стран', 'решения', 'ходе', 'источник']\n","день\t-> ['условиях', 'накануне', 'идет', 'банки']\n","epoch=2 loss.item()=13.772513389587402\n","очередь\t-> ['переговоров', 'отказался', 'возможно', 'края']\n","сегодня\t-> ['стран', 'решения', 'ходе', 'зрения']\n","день\t-> ['условиях', 'накануне', 'банки', 'идет']\n","epoch=3 loss.item()=13.631192207336426\n","очередь\t-> ['переговоров', 'отказался', 'возможно', 'края']\n","сегодня\t-> ['стран', 'ходе', 'решения', 'зрения']\n","день\t-> ['условиях', 'накануне', 'банки', 'идет']\n","epoch=4 loss.item()=13.706318855285645\n","очередь\t-> ['переговоров', 'отказался', 'края', 'возможно']\n","сегодня\t-> ['стран', 'ходе', 'решения', 'зрения']\n","день\t-> ['условиях', 'банки', 'накануне', 'долларов']\n","epoch=5 loss.item()=11.89243221282959\n","очередь\t-> ['переговоров', 'отказался', 'края', 'возможно']\n","сегодня\t-> ['стран', 'ходе', 'решения', 'республике']\n","день\t-> ['условиях', 'банки', 'накануне', 'долларов']\n","epoch=6 loss.item()=10.558237075805664\n","очередь\t-> ['переговоров', 'отказался', 'края', 'возможно']\n","сегодня\t-> ['стран', 'ходе', 'решения', 'республике']\n","день\t-> ['условиях', 'банки', 'накануне', 'идет']\n","epoch=7 loss.item()=9.873285293579102\n","очередь\t-> ['переговоров', 'отказался', 'края', 'возможно']\n","сегодня\t-> ['стран', 'ходе', 'решения', 'республике']\n","день\t-> ['условиях', 'банки', 'накануне', 'идет']\n","epoch=8 loss.item()=8.791401863098145\n","очередь\t-> ['переговоров', 'отказался', 'края', 'возможно']\n","сегодня\t-> ['стран', 'ходе', 'решения', 'размере']\n","день\t-> ['условиях', 'банки', 'накануне', 'идет']\n","epoch=9 loss.item()=7.640630722045898\n","очередь\t-> ['переговоров', 'отказался', 'края', 'возможно']\n","сегодня\t-> ['стран', 'решения', 'ходе', 'размере']\n","день\t-> ['условиях', 'банки', 'накануне', 'идет']\n"]}]},{"cell_type":"markdown","id":"4696acda","metadata":{"id":"4696acda"},"source":["<p class=\"task\" id=\"5\"></p>\n","\n","5\\. Реализуйте класс `NegativeSampler`, который позволяет сгенерировать набор отрицательных примеров. Для генерации отрицательных примеров выбирайте токены пропорционально величине $C(w)^{\\frac{3}{4}}$, где $C(w)$ - частота токена в корпусе.\n","\n","\n","- [ ] Проверено на семинаре"]},{"cell_type":"code","source":["class NegativeSampler:\n","    def __init__(self, corpus):\n","        self.corpus = corpus\n","        self.token_freq = self.calc_token_freq()\n","\n","    def calc_token_freq(self):\n","        token_freq = nltk.FreqDist(self.corpus)\n","        return {token: freq**0.75 for token, freq in token_freq.items()}\n","\n","    def generate_negative(self, num_examples):\n","        prob_dist = nltk.probability.DictionaryProbDist(self.token_freq, normalize=True)\n","        negative_examples = [prob_dist.generate() for _ in range(num_examples)]\n","        return negative_examples\n","\n","corpus = corpus_t[0]\n","sampler = NegativeSampler(corpus_t[0])\n","negative_examples = sampler.generate_negative(4)\n","negative_examples"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"z2aQODsq-axa","executionInfo":{"status":"ok","timestamp":1698578068394,"user_tz":-180,"elapsed":311,"user":{"displayName":"Delovaya Kolbasa","userId":"08122373828115757783"}},"outputId":"39e0bf7b-b68c-454c-d6cd-bd8fb34af529"},"id":"z2aQODsq-axa","execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["['такое', 'администрации', 'сообщили', 'рф']"]},"metadata":{},"execution_count":15}]},{"cell_type":"code","source":["corpus_t[0]"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"oDhD3nMBIlND","executionInfo":{"status":"ok","timestamp":1698577935408,"user_tz":-180,"elapsed":9,"user":{"displayName":"Delovaya Kolbasa","userId":"08122373828115757783"}},"outputId":"189bc429-178f-4c3d-9913-612ac84d5edf"},"id":"oDhD3nMBIlND","execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["['президент',\n"," 'россии',\n"," 'владимир',\n"," 'путин',\n"," 'считает',\n"," 'концепция',\n"," 'реформирования',\n"," 'армии',\n"," 'должна',\n"," 'готова',\n"," 'ноябрю',\n"," 'такое',\n"," 'заявление',\n"," 'сделал',\n"," 'совещанием',\n"," 'членов',\n"," 'совета',\n"," 'безопасности',\n"," 'рф',\n"," 'сообщили',\n"," 'рбк',\n"," 'администрации',\n"," 'президента',\n"," 'путин',\n"," 'также',\n"," 'отметил',\n"," 'эта',\n"," 'реформа',\n"," 'должна',\n"," 'проводиться',\n"," 'учетом',\n"," 'проблем',\n"," 'существующих',\n"," 'настоящее',\n"," 'время',\n"," 'вооруженных',\n"," 'силах',\n"," 'рф']"]},"metadata":{},"execution_count":13}]},{"cell_type":"markdown","id":"983cd8e5","metadata":{"id":"983cd8e5"},"source":["<p class=\"task\" id=\"6\"></p>\n","\n","6\\. Реализуйте и настройте skip-gram модель с использованием negative sampling. Перед началом обучения выберите случайным образом несколько слов из датасета и для каждого из них выведите на экран 3 ближайших слова в смысле косинусной близости между эмбеддингами. В процессе настройки для валидации периодически выводите на экран информацию о ближайших словах для этих слов. Выведите на экран график значения функции потерь в зависимости от номера эпохи.  \n","\n","- [ ] Проверено на семинаре"]},{"cell_type":"markdown","id":"66caa919","metadata":{"id":"66caa919"},"source":["## Обратная связь\n","- [ ] Хочу получить обратную связь по решению"]}],"metadata":{"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.13"},"colab":{"provenance":[],"collapsed_sections":["49cb99c9"]}},"nbformat":4,"nbformat_minor":5}